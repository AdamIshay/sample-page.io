<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Learning to Solve Constraint Satisfaction Problems with Recurrent Transformers</title>
<link href="style.css" rel="stylesheet">
</head>

<body>
<div class="content">
  <h1>Learning to Solve Constraint Satisfaction Problems with Recurrent Transformers</h1>

  <p id="authors"> <a href="#">Author1<sup>* 1,2</sup></a> <a href="#">Author2<sup>* 1,2</sup></a> <a href="#">Author3<sup>1</sup></a>  <br>

  <span style="font-size: 16px"><br>
      <sup>1</sup>Arizona State University <sup>2</sup> Samsung <br>
   <!-- <sup>*</sup>Denotes Equal Contribution</span> -->
      </p>

  <br>
  <img src="./images/recurrence.PNG" class="teaser-gif" style="width:70%;"><br>
    <font size="+2">
          <p style="text-align: center;">
              <a href="https://openreview.net/pdf?id=udNhDCr2KQe">Paper</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://github.com/azreasoners/recurrent_transformer">Code </a>
          </p>
    </font>
</div>



<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p> Constraint satisfaction problems (CSPs) are about finding values of variables that satisfy the given constraints. We show that the Transformer model extended with recurrence is a viable approach to learning to solve CSPs in an end-to-end manner, having clear advantages over the  state-of-the-art methods such as Graph Neural Networks, SATNet, and some neuro-symbolic models. With the ability of Transformers to handle visual input, the proposed Recurrent Transformer can straightforwardly be applied to visual constraint reasoning problems while successfully addressing the symbol grounding problem. We also show how to leverage deductive knowledge of discrete constraints in the Transformer's inductive learning to achieve sample-efficient learning and semi-supervised learning for CSPs.</p>
</div>


<div class="content">


  <h2>Recurrence</h2>
     <p>


<img class="summary-img" src="./images/recurrence.PNG" style="width:50%;"><br>
We use a recurrent encoder-only model without masking attention. This allows the    <p>



</div>
<div class="content">
      <h2>Results</h2>
    <p>
        <img class="summary-img" src="./images/editing_results-01.png" style="width:50%;">
  </p>
</div>


<div class="content">
    <h2>Grounding Visual Sudoku</h2>
    <p>
        We use the ungrounded visual Sudoku dataset (right) which does not leak any supervision of given Sudoku elements. This means that the recurrent transformer must ground the Sudoku element images using just the solution.‚Äù (last row).<br><br>
        <img class="summary-img" src="./images/datasets.PNG" style="width:50%;"><br>
We feed eighty-one 28x28 pixel images of a Sudoku board into the recurrent transformer, which has a small convolutional neural network in the embedding layer.
        <img class="summary-img" src="./images/visual_sudoku.png" style="width:50%;">



  </p>
</div>

<div class="content">
    <h2>Recurrence Analysis</h2>
    <p>

        <img class="summary-img" src="./images/attention.png" style="width:70%;"><br>
              We compare the performance of our full algorithm (green line) to different variations, evaluating the reconstruction quality by measuring the PSNR score as a function of number optimization iterations and running time in minutes. Below, we visually show the inversion results after 200 iterations of our full algorithm (on right) compared to other baselines.<br><br>
        <img class="summary-img" src="./images/recurrent_steps.png" style="width:70%;">
  </p>
</div>

<!--<div class="content">-->
<!--  <h4>BibTex</h4>-->
<!--  <p> @article{???,<br>-->
<!--  &nbsp;&nbsp;title={Null-text inversion for editing real Images using Guided Diffusion Models},<br>-->
<!--  &nbsp;&nbsp;author={Mokady, Ron and Hertz, Amir and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},<br>-->
<!--  &nbsp;&nbsp;booktitle={arXiv preprint arXiv:2211.?????},<br>-->
<!--  &nbsp;&nbsp;year={2022}<br>-->
<!--  } </p>-->
<!--<br>-->

<!--</div>-->

</body>
</html>
